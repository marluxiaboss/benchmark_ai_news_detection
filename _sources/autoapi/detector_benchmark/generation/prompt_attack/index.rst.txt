detector_benchmark.generation.prompt_attack
===========================================

.. py:module:: detector_benchmark.generation.prompt_attack


Classes
-------

.. autoapisummary::

   detector_benchmark.generation.prompt_attack.PromptAttack


Module Contents
---------------

.. py:class:: PromptAttack(gen_model: detector_benchmark.generation.generator.LLMGenerator, gen_config: detector_benchmark.utils.configs.ModelConfig, gen_prompt_config: detector_benchmark.utils.configs.PromptConfig, adversarial_prompt_config: detector_benchmark.utils.configs.PromptConfig, max_sample_len: int, watermarking_scheme: detector_benchmark.watermark.auto_watermark.AutoWatermark = None)

   Bases: :py:obj:`detector_benchmark.generation.article_generator.ArticleGenerator`


   .. py:attribute:: adversarial_prompt_config


   .. py:attribute:: attack_name
      :value: 'prompt_attack'



   .. py:method:: generate_adversarial_text(prefixes: list[str], batch_size: int = 1) -> list[str]

      Generate text with an (adversarial) prompt.

      Parameters:
      ----------
          prefixes: list[str]
              A list of input contexts for text generation.
          batch_size: int
              The batch size to use for generation.

      Returns:
      -------
          list[str]
              A list of generated text.



