detector_benchmark.watermark.synth_id.synth_id
==============================================

.. py:module:: detector_benchmark.watermark.synth_id.synth_id


Classes
-------

.. autoapisummary::

   detector_benchmark.watermark.synth_id.synth_id.SynthIDConfig
   detector_benchmark.watermark.synth_id.synth_id.SynthIDUtils
   detector_benchmark.watermark.synth_id.synth_id.SynthIDState
   detector_benchmark.watermark.synth_id.synth_id.SynthIDLogitsProcessor
   detector_benchmark.watermark.synth_id.synth_id.SynthID


Module Contents
---------------

.. py:class:: SynthIDConfig(algorithm_config: dict, gen_model, model_config, *args, **kwargs)

   Config class for Default Watermark algorithm, load config file and initialize parameters.


   .. py:attribute:: ngram_len


   .. py:attribute:: keys


   .. py:attribute:: sampling_table_size


   .. py:attribute:: sampling_table_seed


   .. py:attribute:: context_history_size


   .. py:attribute:: detector_name


   .. py:attribute:: threshold


   .. py:attribute:: generation_model


   .. py:attribute:: generation_tokenizer


   .. py:attribute:: vocab_size


   .. py:attribute:: device


   .. py:attribute:: gen_kwargs


   .. py:attribute:: top_k


   .. py:attribute:: temperature


.. py:class:: SynthIDUtils(config: SynthIDConfig, *args, **kwargs)

   Utility class for SynthID algorithm, contains helper functions.


   .. py:attribute:: config


   .. py:attribute:: rng


   .. py:method:: accumulate_hash(current_hash: torch.LongTensor, data: torch.LongTensor, multiplier: int = 6364136223846793005, increment: int = 1) -> torch.LongTensor

      Accumulate hash of data on current hash.

      Method uses adapted linear congruential generator with newlib/musl parameters.

      This function has following property -
      f(x, data[T]) = f(f(x, data[:T - 1]), data[T])

      This function expects current_hash.shape and data.shape[:-1] to
      match/broadcastable.

      Args:
          current_hash: (shape,)
          data: (shape, tensor_len)
          multiplier: (int) multiplier of linear congruential generator
          increment: (int) increment of linear congruential generator

      Returns:
          updated hash (shape,)



   .. py:method:: update_scores(scores: torch.FloatTensor, g_values: torch.FloatTensor) -> torch.FloatTensor

      Updates scores using the g values.

      We assume that the scores are in the log space.
      Args:
          scores: Scores (batch_size, vocab_size).
          g_values: G values (batch_size, vocab_size, depth).

      Returns:
          Updated scores (batch_size, vocab_size).



   .. py:method:: update_scores_distortionary(scores: torch.FloatTensor, g_values: torch.FloatTensor, num_leaves: int) -> torch.FloatTensor

      Update scores using the g values for distortionary tournament watermarking.

      We assume that the scores are in the log space.
      Args:
          scores: Scores (batch_size, vocab_size).
          g_values: G values (batch_size, vocab_size, depth).
          num_leaves: Number of leaves per node in the tournament tree.

      Returns:
          Updated scores (batch_size, vocab_size).



   .. py:method:: mean_score_numpy(g_values, mask)

      Args:
          g_values: shape [batch_size, seq_len, watermarking_depth]
          mask: shape [batch_size, seq_len]
      Returns:
          scores: shape [batch_size]



   .. py:method:: weighted_mean_score_numpy(g_values: numpy.ndarray, mask: numpy.ndarray, weights: numpy.ndarray = None) -> numpy.ndarray

      Computes the Weighted Mean score.

      Args:
          g_values: g-values of shape [batch_size, seq_len, watermarking_depth]
          mask: A binary array shape [batch_size, seq_len] indicating which g-values
              should be used. g-values with mask value 0 are discarded
          weights: array of non-negative floats, shape [watermarking_depth]. The
              weights to be applied to the g-values. If not supplied, defaults to
              linearly decreasing weights from 10 to 1

      Returns:
          Weighted Mean scores, of shape [batch_size]. This is the mean of the
          unmasked g-values, re-weighted using weights.



.. py:class:: SynthIDState(batch_size: int, ngram_len: int, context_history_size: int, device: torch.device)

   SynthID watermarking state.


   .. py:attribute:: context


   .. py:attribute:: context_history


   .. py:attribute:: num_calls
      :value: 0



.. py:class:: SynthIDLogitsProcessor(config: SynthIDConfig, utils: SynthIDUtils, *args, **kwargs)

   Bases: :py:obj:`transformers.LogitsProcessor`


   LogitsProcessor for SynthID algorithm, process logits to add watermark.


   .. py:attribute:: config


   .. py:attribute:: utils


   .. py:attribute:: state
      :value: None



   .. py:attribute:: ngram_len


   .. py:attribute:: keys


   .. py:attribute:: sampling_table_size


   .. py:attribute:: context_history_size


   .. py:attribute:: device


   .. py:attribute:: sampling_table


   .. py:method:: __call__(input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor

      Process logits to add watermark.



   .. py:method:: _compute_keys(context: torch.LongTensor, top_k_indices: torch.LongTensor) -> tuple[torch.LongTensor, torch.LongTensor]

      Compute ngram keys for given context and possible next tokens.



   .. py:method:: sample_g_values(ngram_keys: torch.LongTensor) -> torch.LongTensor

      Sample g values from pre-computed sampling table.



   .. py:method:: compute_g_values(input_ids: torch.LongTensor) -> torch.LongTensor

      Computes g values for each ngram from the given sequence of tokens.

      Args:
          input_ids: Input token ids (batch_size, input_len).

      Returns:
          G values (batch_size, input_len - (ngram_len - 1), depth).



   .. py:method:: compute_ngram_keys(ngrams: torch.LongTensor) -> torch.LongTensor

      Computes random keys for each ngram and depth.

      Args:
          ngrams: Ngrams (batch_size, num_ngrams, ngram_len).

      Returns:
          ngram keys (batch_size, num_ngrams, depth).



   .. py:method:: compute_eos_token_mask(input_ids: torch.LongTensor, eos_token_id: int) -> torch.LongTensor

      Computes repetitions mask.

      1 stands for ngrams that don't contain EOS tokens and vice versa.

      Args:
          input_ids: Input token ids (batch_size, input_len).
          eos_token_id: EOS token ID.

      Returns:
          EOS token mask (batch_size, input_len).



   .. py:method:: compute_context_repetition_mask(input_ids: torch.LongTensor) -> torch.LongTensor

      Computes repetition mask.

      0 and 1 stand for repeated and not repeated context n-1 grams respectively.

      Args:
          input_ids: Input token ids (batch_size, input_len).

      Returns:
          Repetitions mask (batch_size, input_len - (ngram_len - 1)).



.. py:class:: SynthID(algorithm_config: str, gen_model, transformers_config: detector_benchmark.utils.configs.ModelConfig, *args, **kwargs)

   Bases: :py:obj:`detector_benchmark.watermark.base.BaseWatermark`


   Top-level class for SynthID algorithm.


   .. py:attribute:: config


   .. py:attribute:: utils


   .. py:attribute:: logits_processor


   .. py:attribute:: detector


   .. py:method:: generate_watermarked_text(prompt: str, *args, **kwargs) -> str

      Generate watermarked text.



   .. py:method:: detect_watermark(text: str, return_dict: bool = True, *args, **kwargs)

      Detect watermark in the text.

      Args:
          text (str): Text to detect watermark in
          return_dict (bool): Whether to return results as dictionary

      Returns:
          Union[Dict[str, Union[bool, float]], Tuple[bool, float]]: Detection results



